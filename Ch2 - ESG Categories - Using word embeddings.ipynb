{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from mai_nlp_helper_functions import *\n",
    "except ImportError as e:\n",
    "    raise ImportError(\"You don't have the mai_nlp_helper_functions.py file in the same directory as your notebook. Either add it, or copy paste the contents in this cell\") from e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "8vW9svTE289D"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Since its inception in 2010, we have distribut...</td>\n",
       "      <td>Social</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I will focus first on traffic, which is a key ...</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Also, many farms are aged with degraded soil t...</td>\n",
       "      <td>Environmental</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alaska 2009 STEWARDSHIP REPORT54 cOnSERvATIOn,...</td>\n",
       "      <td>Social</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The company’s U.S. GOM operations can be impac...</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2180</th>\n",
       "      <td>American shad are a species of concern for res...</td>\n",
       "      <td>Environmental</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2181</th>\n",
       "      <td>Weyerhaeuser's policies address best practices...</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2182</th>\n",
       "      <td>compares recently observed trends in weather p...</td>\n",
       "      <td>Environmental</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2183</th>\n",
       "      <td>If our partners, including our licensors, suff...</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184</th>\n",
       "      <td>Subsequently through three separate acquisitio...</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2185 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text        subject\n",
       "0     Since its inception in 2010, we have distribut...         Social\n",
       "1     I will focus first on traffic, which is a key ...          Other\n",
       "2     Also, many farms are aged with degraded soil t...  Environmental\n",
       "3     Alaska 2009 STEWARDSHIP REPORT54 cOnSERvATIOn,...         Social\n",
       "4     The company’s U.S. GOM operations can be impac...          Other\n",
       "...                                                 ...            ...\n",
       "2180  American shad are a species of concern for res...  Environmental\n",
       "2181  Weyerhaeuser's policies address best practices...          Other\n",
       "2182  compares recently observed trends in weather p...  Environmental\n",
       "2183  If our partners, including our licensors, suff...          Other\n",
       "2184  Subsequently through three separate acquisitio...          Other\n",
       "\n",
       "[2185 rows x 2 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"https://media.githubusercontent.com/media/JosPolfliet/vlerick-mai-nlp-2023/main/DATA/esg_reports.csv\")\n",
    "df[\"subject\"] = df[\"subject\"].fillna(\"Other\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(sentence):\n",
    "    \"\"\"\n",
    "    Tokenises a sentence using spaCy.\n",
    "    Parameters:\n",
    "    - sentence: str, the sentence to tokenise\n",
    "    Returns:\n",
    "    - mytokens: list, the list of tokens\n",
    "    \"\"\"\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    global nlp\n",
    "    global stop_words\n",
    "    if not nlp:\n",
    "        try:\n",
    "            nlp = spacy.load(\"en_core_web_trf\")\n",
    "            stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "        except:\n",
    "            spacy.cli.download(\"en_core_web_trf\")\n",
    "            nlp = spacy.load(\"en_core_web_trf\")\n",
    "            stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "    tokens = nlp(sentence[\"text\"].lower())\n",
    "\n",
    "    # Remove OOV words\n",
    "    tokens = [word for word in tokens if not word.is_oov]\n",
    "\n",
    "    # Lemmatise + lower case\n",
    "    tokens = [\n",
    "        word.lemma_.strip() if word.lemma_ != \"-PRON-\" else word.lower_\n",
    "        for word in tokens\n",
    "    ]\n",
    "\n",
    "    # Remove stop words\n",
    "    # tokens = [\n",
    "    #     word for word in tokens if word not in stop_words and word not in punctuations\n",
    "    # ]\n",
    "\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The geographic analysis in the Business Review, including the average balance sheet and interest rates, changes in net interest income and average interest rates, yields, spreads and margins in this report have generally been compiled on the basis of location of office - UK and overseas – unless indicated otherwise.\n"
     ]
    }
   ],
   "source": [
    "print(sentence.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "geographic\n",
      "analysis\n",
      "in\n",
      "the\n",
      "business\n",
      "review\n",
      ",\n",
      "include\n",
      "the\n",
      "average\n",
      "balance\n",
      "sheet\n",
      "and\n",
      "interest\n",
      "rate\n",
      ",\n",
      "change\n",
      "in\n",
      "net\n",
      "interest\n",
      "income\n",
      "and\n",
      "average\n",
      "interest\n",
      "rate\n",
      ",\n",
      "yield\n",
      ",\n",
      "spread\n",
      "and\n",
      "margin\n",
      "in\n",
      "this\n",
      "report\n",
      "have\n",
      "generally\n",
      "be\n",
      "compile\n",
      "on\n",
      "the\n",
      "basis\n",
      "of\n",
      "location\n",
      "of\n",
      "office\n",
      "-\n",
      "uk\n",
      "and\n",
      "overseas\n",
      "–\n",
      "unless\n",
      "indicate\n",
      "otherwise\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in spacy_tokenizer({\"text\": \"The geographic analysis in the Business Review, including the average balance sheet and interest rates, changes in net interest income and average interest rates, yields, spreads and margins in this report have generally been compiled on the basis of location of office - UK and overseas – unless indicated otherwise.\"}):\n",
    "    print(token)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.1043    2.3496    3.2472   ... -7.6875   -2.5128    0.69342 ]\n",
      " [-3.0683    0.8136   -0.29935  ... -2.0391   -0.9999    1.9089  ]\n",
      " [ 0.65773  -1.1594    1.1196   ... -3.4405    2.6652    0.34709 ]\n",
      " ...\n",
      " [-2.0586    0.34812   0.84462  ... -3.4597   -2.9688    1.4875  ]\n",
      " [-0.6902    0.59066  -1.8021   ... -1.4649   -2.0703    4.1519  ]\n",
      " [-0.076454 -4.6896   -4.0431   ...  1.304    -0.52699  -1.3622  ]]\n",
      "(55, 300)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def embed_words(sentence):\n",
    "    tokens = spacy_tokenizer(sentence)\n",
    "    return nlp.vocab.vectors.get_batch(tokens)\n",
    "sentence = df.iloc[301]\n",
    "\n",
    "word_embeddings = embed_words(sentence)\n",
    "print(word_embeddings)\n",
    "print(word_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple model: just average the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_sentence_simple(sentence):\n",
    "    \"\"\"\n",
    "    This is the part where you can go crazy and complex to add features, change aggregation way, use PCA, ...\n",
    "    \"\"\"\n",
    "    word_embeddings = embed_words(sentence)\n",
    "    # sentence_embedding = np.concatenate([word_embeddings.min(axis=0), \n",
    "    #                                      word_embeddings.max(axis=0), \n",
    "    #                                      [embed_words(sentence).shape[0], \n",
    "    #                                      len([token for token in nlp(sentence[\"text\"]) if token.is_oov])]])\n",
    "    # sentence_embedding = np.concatenate([word_embeddings.min(axis=0), word_embeddings.max(axis=0)])\n",
    "    sentence_embedding = word_embeddings.mean(axis=0)\n",
    "    return sentence_embedding\n",
    "\n",
    "sentence_embedding = embed_sentence_simple(sentence)\n",
    "print(sentence_embedding)\n",
    "print(sentence_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df[\"features_simple\"] = df.progress_apply(embed_sentence_simple,axis=1)\n",
    "\n",
    "# For bigger datasets, you can write the results to disk to cache them\n",
    "# df.to_pickle(\"DATA/df_with_features.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cached results\n",
    "# df = pd.read_pickle(\"DATA/df_with_features.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack(df[\"features_simple\"].values)\n",
    "y = np.array(df[\"subject\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=22141)\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=3, random_state=22141, class_weight=\"balanced\", n_estimators=500)\n",
    "clf.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "experiment_name = input(\"Enter experiment name: \")\n",
    "predictions = clf.predict(X_test)\n",
    "stats = evaluate_model(y_test, predictions, clf.classes_)\n",
    "log_experiment_results(experiment_name, stats[\"macro avg\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "The example above is to illustrate the theory of embeddings. In reality, you can build pipelines super easily with SpaCy (if you prioritize speed) or HuggingFace transfomers (if you prioritize accuracy)\n",
    "\n",
    "https://medium.com/@ycouble/training-and-integrating-a-custom-text-classifier-to-a-spacy-pipeline-b19e6a132487"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Vlerick-MAI-NLP-demo.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
